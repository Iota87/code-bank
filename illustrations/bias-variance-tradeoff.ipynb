{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Illustration**: Bias-Variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustration shows how __Bias__ and __Variance__ change as a linear regression is expanded with polynomial terms. \n",
    "\n",
    "We first define a __Data Generating Process__ (DGP), and then draw repeated samples from the DGP, to show the degree of __Bias__ and __Variance__ of the model at a given value of X -- and then how those vary as the model is made more complex through additional polynomial expansion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all packages \n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import math \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all constants\n",
    "\n",
    "DRAWS     = 10    # number of draws\n",
    "X0        = 3     # value of the arbitrary point where bias will be measured\n",
    "NOISE     = 0.2   # offset for draw from uniform distribution \n",
    "DEGREE    = 5     # highest (up to) degree of polynomial \n",
    "\n",
    "# plotting\n",
    "SLEEP      = 0.75 # sleep time in seconds\n",
    "FONTSIZE   = 20\n",
    "OFFSET     = 0.1  # plot offset on x and y axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot an example\n",
    "def myPlot(X, y, y_pred, draw, symbol = 'o'):\n",
    "    \n",
    "    font = {'size'   : FONTSIZE}\n",
    "    matplotlib.rc('font', **font)\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    plt.plot(X, y, symbol, markersize=12, linewidth=3, label='Ground truth')\n",
    "    plt.ylim(0 - OFFSET, 1 + OFFSET)\n",
    "    plt.xlim(min(X) - OFFSET, max(X) + OFFSET)\n",
    "    plt.hlines(0, xmin = min(X), xmax = max(X), colors='black', linewidth=3)\n",
    "    plt.hlines(1, xmin = min(X), xmax = max(X), colors='black', linewidth=3)\n",
    "    \n",
    "    if type(y_pred) == float:\n",
    "        plt.hlines(y_pred, xmin = min(X), xmax = max(X), colors='red', linestyles='dashed', linewidth=3)\n",
    "    if type(y_pred) == np.ndarray:\n",
    "        plt.plot(X, y_pred, '-', markersize = 12, linewidth=3, color = 'red')\n",
    "        \n",
    "    plt.ylabel('y')\n",
    "    plt.xlabel('X')\n",
    "    plt.title('Draw: {}'.format(draw))\n",
    "    \n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Define the data generating process (DGP)\n",
    "\n",
    "We define the DGP to be the logistic sigmoid function, as shown below:\n",
    "\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the DGP\n",
    "def sigmoid(x, noise = True):\n",
    "    \n",
    "    y = 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    # add noise\n",
    "    if noise:\n",
    "        y += np.random.uniform(0 - NOISE, 0 + NOISE)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one draw from the DGP\n",
    "X = list(range(-50, 50))\n",
    "X = [i/10 for i in X]\n",
    "y = [sigmoid(i) for i in X]\n",
    "myPlot(X, y, None, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Repeatedly resample from DGP and measure bias/variance at an arbitrary point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Our most simple prediction: __The Mean__\n",
    "\n",
    "The simplest \"model\" predicts the mean of the outcome variable. The estimates have very high bias, very low variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_variance = dict()\n",
    "\n",
    "biases = []\n",
    "estimates = []\n",
    "\n",
    "# animate draws and estimates\n",
    "for draw in range(DRAWS):\n",
    "        \n",
    "    # draw new data\n",
    "    y = [sigmoid(i) for i in X]\n",
    "    \n",
    "    # estimate simple mean\n",
    "    y_pred = sum(y)/len(y)\n",
    "    biases.append(y_pred - sigmoid(X0))\n",
    "    estimates.append(y_pred)\n",
    "\n",
    "    # plot draw \n",
    "    p = myPlot(X, y, False, draw)\n",
    "    for est in estimates[:-1]:\n",
    "        p.hlines(est, xmin = min(X), xmax = max(X), colors='gray', linestyles='dashed', linewidth=2)\n",
    "    p.show()\n",
    "    \n",
    "    plt.pause(SLEEP)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # plot estimate \n",
    "    p = myPlot(X, y, y_pred, draw)\n",
    "    for est in estimates[:-1]:\n",
    "        p.hlines(est, xmin = min(X), xmax = max(X), colors='gray', linestyles='dashed', linewidth=2)\n",
    "    p.show()\n",
    "    \n",
    "    plt.pause(SLEEP)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "# plot all estimates\n",
    "\n",
    "y = [sigmoid(i, False) for i in X]\n",
    "plt = myPlot(X, y, False, 'all estimates', '-')\n",
    "\n",
    "for est in estimates:\n",
    "    plt.hlines(est, xmin = min(X), xmax = max(X), colors='red', linestyles='dashed', linewidth=3)\n",
    "    \n",
    "plt.vlines(X0, ymin = min(y), ymax = max(y), colors='black', linestyles='dashed', label = 'x0', linewidth=3)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Bias at point X0: {}'.format(round(abs(np.mean(biases)), 4)))\n",
    "print('Variance at point X0: {}'.format(round(np.var(estimates), 4)))\n",
    "\n",
    "# add to global dictionary\n",
    "bias_variance[0] = ('mean bias', 'variance')\n",
    "bias_variance['mean'] = (round(abs(np.mean(biases)), 4), round(np.var(estimates), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) A slightly more complex prediction: __A Line__ \n",
    "\n",
    "A linear regression model fits a linear line of best fit. The estimates have high bias, moderate variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = []\n",
    "estimates = []\n",
    "\n",
    "# animate draws and estimates\n",
    "\n",
    "for draw in range(DRAWS):\n",
    "        \n",
    "    # draw new data\n",
    "    y = [sigmoid(i) for i in X]\n",
    "    \n",
    "    # estimate simple mean\n",
    "    model = LinearRegression().fit(np.array(X).reshape(-1, 1), y)\n",
    "    y_pred = model.predict(np.array(X).reshape(-1, 1))\n",
    "    \n",
    "    biases.append(y_pred[X.index(X0)] - sigmoid(X0))\n",
    "    estimates.append(y_pred)\n",
    "\n",
    "    # plot draw \n",
    "    p = myPlot(X, y, False, draw)\n",
    "    for est in estimates[:-1]:\n",
    "        plt.plot(X, est, '-', markersize = 12, linewidth=2, color = 'gray', linestyle='dashed')\n",
    "    p.show()\n",
    "    \n",
    "    plt.pause(SLEEP)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # plot estimate \n",
    "    p = myPlot(X, y, y_pred, draw)\n",
    "    for est in estimates[:-1]:\n",
    "        plt.plot(X, est, '-', markersize = 12, linewidth=2, color = 'gray', linestyle='dashed')\n",
    "    p.show()\n",
    "    \n",
    "    plt.pause(SLEEP)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "# plot all estimates\n",
    "\n",
    "y = [sigmoid(i, False) for i in X]\n",
    "plt = myPlot(X, y, False, 'all estimates', '-')\n",
    "\n",
    "for est in estimates:\n",
    "    plt.plot(X, est, '-', markersize = 12, linewidth=3, color = 'red', linestyle='dashed')\n",
    "    \n",
    "plt.vlines(X0, ymin = min(y), ymax = max(y), colors='black', linestyles='dashed', label = 'x0', linewidth=3)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Bias at point X0: {}'.format(round(abs(np.mean(biases)), 4)))\n",
    "print('Variance at point X0: {}'.format(round(np.var(estimates), 4)))\n",
    "\n",
    "# add to global dictionary\n",
    "bias_variance['line'] = (round(abs(np.mean(biases)), 4), round(np.var(estimates), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) An even more complex prediction: __A Higher-Order Polynomial__\n",
    "\n",
    "A nonlinear model has the most flexibility to fit its parameters to the data. The estimates have low bias, very high variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = []\n",
    "estimates = []\n",
    "\n",
    "# animate draws and estimates\n",
    "\n",
    "for draw in range(DRAWS):\n",
    "        \n",
    "    # draw new data\n",
    "    y = [sigmoid(i) for i in X]\n",
    "    \n",
    "    # estimate polynomial \n",
    "    polynomial = PolynomialFeatures(degree=DEGREE, include_bias=True)\n",
    "    \n",
    "    X_ = polynomial.fit_transform(np.array(X).reshape(-1, 1))\n",
    "    model = LinearRegression().fit(X_, y)\n",
    "    y_pred = model.predict(np.array(X_))\n",
    "    \n",
    "    biases.append(y_pred[X.index(X0)] - sigmoid(X0))\n",
    "    \n",
    "    # plot estimate \n",
    "    # compute smooth estimation lines\n",
    "    X_smooth = list(range(-500, 491, 1))\n",
    "    X_smooth = [i/100 for i in X_smooth]\n",
    "    y_polynomial = [sigmoid(i) for i in X_smooth]\n",
    "    X_polynomial = polynomial.transform(np.array(X_smooth).reshape(-1, 1))  # fit on only 10 datapoints\n",
    "    y_polynomial_pred = model.predict(X_polynomial)\n",
    "    estimates.append(y_polynomial_pred)\n",
    "\n",
    "    # plot draw \n",
    "    p = myPlot(X, y, False, draw)\n",
    "    for est in estimates[:-1]:\n",
    "        p.plot(X_smooth, est, '-', markersize = 12, linewidth=2, color = 'gray', linestyle='dashed')\n",
    "    p.show()\n",
    "    \n",
    "    plt.pause(SLEEP)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # fill y's with zeros (this is a hack)\n",
    "    y_ = []\n",
    "    for i in y:\n",
    "        y_.append(i)\n",
    "        if len(y_) < len(X_smooth):\n",
    "            for j in range(9):\n",
    "                y_.append(-1)\n",
    "    # plot\n",
    "    p = myPlot(X_smooth, y_, y_polynomial_pred, draw)\n",
    "    for est in estimates[:-1]:\n",
    "        p.plot(X_smooth, est, '-', markersize = 12, linewidth=2, color = 'gray', linestyle='dashed')\n",
    "    p.show()\n",
    "    \n",
    "    plt.pause(SLEEP)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "# plot all estimates\n",
    "\n",
    "y = [sigmoid(i, False) for i in X_smooth]\n",
    "plt = myPlot(X_smooth, y, False, 'all estimates', '-')\n",
    "\n",
    "for est in estimates:\n",
    "    plt.plot(X_smooth, est, '-', markersize = 12, linewidth=3, color = 'red', linestyle='dashed')\n",
    "    \n",
    "plt.vlines(X0, ymin = min(y), ymax = max(y), colors='black', linestyles='dashed', label = 'x0', linewidth=3)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Bias at point X0: {}'.format(round(abs(np.mean(biases)), 4)))\n",
    "print('Variance at point X0: {}'.format(round(np.var(estimates), 4)))\n",
    "\n",
    "# add to global dictionary\n",
    "bias_variance['polynomial'] = (round(abs(np.mean(biases)), 4), round(np.var(estimates), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Summarize mean bias and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabulate mean bias and variance\n",
    "pd.DataFrame.from_dict(bias_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
