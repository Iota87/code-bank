{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DSFM Demo**: Credit Default - Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: [Data Science for Managers - EPFL Program](https://www.dsfm.ch)  \n",
    "Source:  [https://github.com/dsfm-org/code-bank.git](https://github.com/dsfm-org/code-bank.git)  \n",
    "License: [MIT License](https://opensource.org/licenses/MIT). See open source [license](LICENSE) in the Code Bank repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we try to predict the probability of default on credit card bills using a dataset of customers payments from a Taiwanese bank. A credit default happens when you fail to pay the minimum payment by the due date mentioned on your credit card bill for more than 6 months. From a risk management perspective, the accuracy of the predicted probability of default is more valuable than just a binary prediction (classification) of default itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://greendayonline.com/wp-content/uploads/2017/03/Recovering-From-Student-Loan-Default.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Image: https://greendayonline.com/wp-content/uploads/2017/03/Recovering-From-Student-Loan-Default.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Credit Card Default Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to predict the probability of defaulting on a credit card account at a Taiwanese bank. A credit card default happens when a customer fails to pay the minimum due on a credit card bill for more than 6 months. \n",
    "\n",
    "We will use a dataset from a Taiwanese bank with 30,000 observations (Source: *Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.*). Each observation represents an account at the bank at the end of October 2005.  We renamed the variable default_payment_next_month to customer_default. The target variable to predict is `customer_default` -- i.e., whether the customer will default in the following month (1 = Yes or 0 = No). The dataset also includes 23 other explanatory features. \n",
    "\n",
    "Variables are defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature name     | Variable Type | Description \n",
    "|------------------|---------------|--------------------------------------------------------\n",
    "| customer_default | Binary        | 1 = default in following month; 0 = no default \n",
    "| LIMIT_BAL        | Continuous    | Credit limit   \n",
    "| SEX              | Categorical   | 1 = male; 2 = female\n",
    "| EDUCATION        | Categorical   | 1 = graduate school; 2 = university; 3 = high school; 4 = others\n",
    "| MARRIAGE         | Categorical   | 0 = unknown; 1 = married; 2 = single; 3 = others\n",
    "| AGE              | Continuous    | Age in years  \n",
    "| PAY1             | Categorical   | Repayment status in September, 2005 \n",
    "| PAY2             | Categorical   | Repayment status in August, 2005 \n",
    "| PAY3             | Categorical   | Repayment status in July, 2005 \n",
    "| PAY4             | Categorical   | Repayment status in June, 2005 \n",
    "| PAY5             | Categorical   | Repayment status in May, 2005 \n",
    "| PAY6             | Categorical   | Repayment status in April, 2005 \n",
    "| BILL_AMT1        | Continuous    | Balance in September, 2005  \n",
    "| BILL_AMT2        | Continuous    | Balance in August, 2005  \n",
    "| BILL_AMT3        | Continuous    | Balance in July, 2005  \n",
    "| BILL_AMT4        | Continuous    | Balance in June, 2005 \n",
    "| BILL_AMT5        | Continuous    | Balance in May, 2005  \n",
    "| BILL_AMT6        | Continuous    | Balance in April, 2005  \n",
    "| PAY_AMT1         | Continuous    | Amount paid in September, 2005\n",
    "| PAY_AMT2         | Continuous    | Amount paid in August, 2005\n",
    "| PAY_AMT3         | Continuous    | Amount paid in July, 2005\n",
    "| PAY_AMT4         | Continuous    | Amount paid in June, 2005\n",
    "| PAY_AMT5         | Continuous    | Amount paid in May, 2005\n",
    "| PAY_AMT6         | Continuous    | Amount paid in April, 2005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurement scale for repayment status is:   \n",
    "\n",
    "    -2 = payment two months in advance   \n",
    "    -1 = payment one month in advance   \n",
    "    0 = pay duly   \n",
    "    1 = payment delay for one month   \n",
    "    2 = payment delay for two months   \n",
    "    3 = payment delay for three months   \n",
    "    4 = payment delay for four months   \n",
    "    5 = payment delay for five months   \n",
    "    6 = payment delay for six months   \n",
    "    7 = payment delay for seven months   \n",
    "    8 = payment delay for eight months   \n",
    "    9 = payment delay for nine months or more  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup\n",
    "\n",
    "Put all import statements, constants and helper functions at the top of your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import itertools\n",
    "\n",
    "# Visualization packages \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "from sklearn.metrics         import confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.dummy           import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Special code to ignore un-important warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "FIGSIZE = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to visualize the confusion matrix\n",
    "# Note optional parameter for normalization; apply normalization by setting `normalize=True`\n",
    "\n",
    "def plot_confusion_matrix(cm, classes=[0,1], normalize=False, title='Confusion Matrix', cmap=plt.cm.Reds):\n",
    "    \"\"\" \n",
    "    Function to plot a sklearn confusion matrix, showing number of cases per prediction condition \n",
    "    \n",
    "    Args:\n",
    "        cm         an sklearn confusion matrix\n",
    "        classes    levels of the class being predicted; default to binary outcome\n",
    "        normalize  apply normalization by setting `normalize=True`\n",
    "        title      title for the plot\n",
    "        cmap       color map\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    plt.imshow(cm, aspect='auto', interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.locator_params(nbins=2)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # add FP, TP, FN, TN counts\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round (cm[i, j],2), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Data Preprocessing and EDA\n",
    "\n",
    "First, we would like to understand the main characteristics of the dataset. We might need to transform and clean some features before we can specify a statistical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a dataframe\n",
    "data = pd.read_csv('credit_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of data in the form of (number of observations, number of features)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of an arbitrary column\n",
    "data['PAY_1'].hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to check distribution of value\n",
    "data['MARRIAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ovrall statistics about data frame\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate 'EDUCATION' field \n",
    "# Notice the mismatch between data description and actual data !\n",
    "data['EDUCATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tab 'MARRIAGE' and the target \n",
    "pd.crosstab(data['MARRIAGE'], data['customer_default'], margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of target feature\n",
    "data['customer_default'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataframe into two data frames, with and without credit default\n",
    "data_def_1 = data[data['customer_default'] == 1]\n",
    "data_def_0 = data[data['customer_default'] == 0]\n",
    "\n",
    "# Check distribution of different features in with and without default dataframes as well as full dataframe\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.style.use('seaborn-deep')\n",
    "col = 'LIMIT_BAL'\n",
    "plt.xlabel(col)\n",
    "plt.ylabel('count')\n",
    "plt.hist([data_def_0[col], data_def_1[col], data[col]], label=['non_default-'+col, 'default-'+col, 'all-'+col])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of different features using box plot\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "data.boxplot(column='LIMIT_BAL')\n",
    "plt.ylabel('Feature value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize corelation of different features with respect to each other\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = data.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Setup the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of one feature with respect to another one\n",
    "sns.jointplot(x = \"LIMIT_BAL\", y = \"customer_default\", data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pair-wise distribution of a set of features\n",
    "sample_data = data.iloc[:,1:5]\n",
    "sns.pairplot(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (the automated approach)\n",
    "\n",
    "Instead of performing all of the steps above manually, you can also run a \"profile\" on the dataset first, and then drill down into specific cases of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the automated pandas profiling utility to examine the dataset\n",
    "# data.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Data Preprocessing and Designing Cross-validation Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among different features, SEX and MARRIAGE are categoical while others are either numerical or ordinal with acceptable labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of sex and marital status\n",
    "# Applying one-hot encoding for other field doesn't make sense since they are either numeric or ordinal\n",
    "cols_to_transform = ['SEX', 'MARRIAGE']\n",
    "data_with_dummies = pd.get_dummies(data=data, columns = cols_to_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "cols_X = [ 'LIMIT_BAL', 'EDUCATION', 'AGE', 'PAY_1', 'PAY_2', 'PAY_3',\n",
    "           'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3',\n",
    "           'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2',\n",
    "           'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'SEX_1', 'SEX_2', 'MARRIAGE_0',\n",
    "           'MARRIAGE_1', 'MARRIAGE_2', 'MARRIAGE_3']\n",
    "col_y  =   'customer_default'\n",
    "\n",
    "X = data_with_dummies.loc[:, cols_X]\n",
    "y = data_with_dummies[col_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape , X_test.shape , y_train.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Classification using Dummy Classifier and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Baseline\n",
    "\n",
    "A good practice is to start with a \"dumb\" model that simply predicts the average (for a regression) or the most frequent outcome (for classification). That gives you sense of how well a complex model performs compared to the most simple model. As this is a classification problem, we will start by predicting No Default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a baseline using dummy classifier that predicts the most frequent label\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict default - as a binary outcome and a probability between 0 and 1\n",
    "y_pred_dummy       = dummy_clf.predict(X_test)\n",
    "y_pred_dummy_proba = dummy_clf.predict_proba(X_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm_dummy = confusion_matrix(y_test, y_pred_dummy)\n",
    "plot_confusion_matrix(cm_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many errors did the dumb model make?\n",
    "errors_dummy = cm_dummy[0][1] + cm_dummy[1][0]\n",
    "print('Num Errors =', errors_dummy, '\\n')\n",
    "print('Accuracy   =', \"{0:.4f}\".format(float((sum(cm_dummy)[0] - errors_dummy)/sum(cm_dummy)[0])))\n",
    "print('AUC Score  =', \"{0:.4f}\".format(roc_auc_score(y_test, y_pred_dummy_proba[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Logit model (no regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification using a logit model\n",
    "lr_clf = LogisticRegression(C=10e9)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict default - as a binary outcome and a probability between 0 and 1\n",
    "y_pred_lr       = lr_clf.predict(X_test)\n",
    "y_pred_lr_proba = lr_clf.predict_proba(X_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "plot_confusion_matrix(cm_lr,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many errors did the dumb model make?\n",
    "errors_lr = cm_lr[0][1] + cm_lr[1][0]\n",
    "print('Num Errors =', errors_lr, '\\n')\n",
    "print('Accuracy   =', \"{0:.4f}\".format(float((sum(cm_lr)[0] - errors_lr)/sum(cm_lr)[0])))\n",
    "print('AUC Score  =', \"{0:.4f}\".format(roc_auc_score(y_test, y_pred_lr_proba[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary of performance metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 30\n",
    "models = ['Dummy', 'Logit']\n",
    "results = [float((sum(cm_dummy)[0] - errors_dummy)/sum(cm_dummy)[0]), float((sum(cm_lr)[0] - errors_lr)/sum(cm_lr)[0])]\n",
    "print('', '=' * width, '\\n', 'Summary of Accuracy Scores'.center(width), '\\n', '=' * width)  \n",
    "for i in range(len(models)):\n",
    "    print(models[i].center(width-8), '{0:.4f}'.format(results[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 25\n",
    "models = ['Dummy', 'Logit']\n",
    "results = [roc_auc_score(y_test, y_pred_dummy_proba[:, 1]), roc_auc_score(y_test, y_pred_lr_proba[:, 1])]\n",
    "print('', '=' * width, '\\n', 'Summary of AUC Scores'.center(width), '\\n', '=' * width)  \n",
    "for i in range(len(models)):\n",
    "    print(models[i].center(width-8), '{0:.4f}'.format(results[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4**: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the logit model did WORSE than the baseline model above (at least when predicting class labels with a default probability threshold and when evaluated by the total number of errors made by the model). Using accuracy to compare models, however, as two drawbacks for this credit default problem:  \n",
    "\n",
    "  1. The distribution of the target outcome variable is not balanced. Therefore, a model that uniformly predicts that NO customer will default can still get a high accuracy.\n",
    "\n",
    "  2. Different probability thresholds for making a \"positive\" prediction can lead to different performance results in the model. Thus, it may be more sensible to configure the model to predict and use probabilities, instead of directly predicting a label.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more useful metric for classification is the \"**Area under the ROC curve**\" (also called the **AUC**). The AUC gives you the probability that a randomly selected customer that is labeled as defaulting will be assigned a higher probability of default than a randomly selected customer that is labeled as NOT defaulting. The AUC also does not suffer from class skewness, and therefore is able to adjust for probabilities directly (instead of the distribution of class labels). But looking above, how do AUC scores actually compare...?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do we need to standardize the data?**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not necessary to normalize or standardize data for linear models when there is no regularization or special optimization procedure involved. Estimated coefficients will adjust to non-normalized data and the accuracy of the prediction will not be affected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do we need to use a _Pipeline_?** (What is a Pipeline, anyway?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized models and other, more-complex models often require **standardized data** to work properly. However, you cannot standardize all of the data up-front in the beginning; if you do, then information will \"leak\" from the training data into the testing data because the standardization will reflect both datasets. You therefore need a special way to process data and models so that processing steps can be defined up-front, but run later on only the relevant sub-section of data. You accomplish that by using a late-execution \"**pipeline**.\" _Pipelines can be confusing at first, but eventually you will get the hang of it._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Was our training/testing method robust?** \n",
    "\n",
    "One concern with the previous training/testing approach is that the relative performance of the baseline model and the Logit model might depend on the particular training/testing split in the data that was performed. In a more advanced analysis, we might use k-fold cross-validation for a more reliable testing estimate. Another concern with the previous training/testing approach arises when we need to \"tune\" a model with a hyper-parameter for better performance. In that case, hyper-parameter tuning would pick up information from the testing data and overfit the model. In the models that follow, we will need to tune hyper-parameters, so we will now need to change our testing strategy and generate three data splits: **training** (60%), **validation** (20%) and **test** (20%). We can do this by first splitting a training/testing set 80/20, and then further split the training set into a smaller training set and a validation set. The terminology for these sets can be confusing (and different resources use different names for the sets), but in this example we will create a \"Validation Set\" that pulls from what was originally part of the training set. Most importantly, one should set aside a testing set that is *never used* until the very final test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
